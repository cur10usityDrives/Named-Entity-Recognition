{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPjlp2nBXNLFKicQsWKeJPc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cur10usityDrives/Named-Entity-Recognition/blob/main/named_entity_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pEKf9NYzrPL0",
        "outputId": "9754b557-0604-4d8a-dd0c-4857d463b441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'Beirut', ',', 'a', 'string', 'of', 'officials', 'voiced', 'their', 'anger', ',', 'while', 'at', 'the', 'United', 'Nations', 'summit', 'in', 'New', 'York', ',', 'Prime', 'Minister', 'Fouad', 'Siniora', 'said', 'the', 'Lebanese', 'people', 'are', 'resolute', 'in', 'preventing', 'such', 'attempts', 'from', 'destroying', 'their', 'spirit', '.']\n",
            "Person Names: ['Siniora']\n",
            "Country Names: []\n",
            "Organization Names: []\n",
            "Official Titles: []\n",
            "Datetimes: []\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import spacy\n",
        "import pandas as pd\n",
        "\n",
        "# Load the English NER model from spaCy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Function to extract person names using spaCy NER\n",
        "def extract_person_names(text):\n",
        "    doc = nlp(text)\n",
        "    person_names = [ent.text for ent in doc.ents if ent.label_ == 'PERSON']\n",
        "    return person_names\n",
        "\n",
        "# Function to extract country names\n",
        "def extract_country_names(text):\n",
        "    countries = ['Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Antigua and Barbuda', 'Argentina', 'Armenia', 'Australia', 'Austria', 'Azerbaijan', 'Bahamas', 'Bahrain', 'Bangladesh', 'Barbados', 'Belarus', 'Belgium', 'Belize', 'Benin', 'Bhutan', 'Bolivia', 'Bosnia and Herzegovina', 'Botswana', 'Brazil', 'Brunei', 'Bulgaria', 'Burkina Faso', 'Burundi', 'Cabo Verde', 'Cambodia', 'Cameroon', 'Canada', 'Central African Republic', 'Chad', 'Chile', 'China', 'Colombia', 'Comoros', 'Congo', 'Costa Rica', 'Croatia', 'Cuba', 'Cyprus', 'Czech Republic', 'Democratic Republic of the Congo', 'Denmark', 'Djibouti', 'Dominica', 'Dominican Republic', 'East Timor', 'Ecuador', 'Egypt', 'El Salvador', 'Equatorial Guinea', 'Eritrea', 'Estonia', 'Eswatini', 'Ethiopia', 'Fiji', 'Finland', 'France', 'Gabon', 'Gambia', 'Georgia', 'Germany', 'Ghana', 'Greece', 'Grenada', 'Guatemala', 'Guinea', 'Guinea-Bissau', 'Guyana', 'Haiti', 'Honduras', 'Hungary', 'Iceland', 'India', 'Indonesia', 'Iran', 'Iraq', 'Ireland', 'Israel', 'Italy', 'Ivory Coast', 'Jamaica', 'Japan', 'Jordan', 'Kazakhstan', 'Kenya', 'Kiribati', 'Kosovo', 'Kuwait', 'Kyrgyzstan', 'Laos', 'Latvia', 'Lebanon', 'Lesotho', 'Liberia', 'Libya', 'Liechtenstein', 'Lithuania', 'Luxembourg', 'Madagascar', 'Malawi', 'Malaysia', 'Maldives', 'Mali', 'Malta', 'Marshall Islands', 'Mauritania', 'Mauritius', 'Mexico', 'Micronesia', 'Moldova', 'Monaco', 'Mongolia', 'Montenegro', 'Morocco', 'Mozambique', 'Myanmar', 'Namibia', 'Nauru', 'Nepal', 'Netherlands', 'New Zealand', 'Nicaragua', 'Niger', 'Nigeria', 'North Korea', 'North Macedonia', 'Norway', 'Oman', 'Pakistan', 'Palau', 'Palestine', 'Panama', 'Papua New Guinea', 'Paraguay', 'Peru', 'Philippines', 'Poland', 'Portugal', 'Qatar', 'Romania', 'Russia', 'Rwanda', 'Saint Kitts and Nevis', 'Saint Lucia', 'Saint Vincent and the Grenadines', 'Samoa', 'San Marino', 'Sao Tome and Principe', 'Saudi Arabia', 'Senegal', 'Serbia', 'Seychelles', 'Sierra Leone', 'Singapore', 'Slovakia', 'Slovenia', 'Solomon Islands', 'Somalia', 'South Africa', 'South Korea', 'South Sudan', 'Spain', 'Sri Lanka', 'Sudan', 'Suriname', 'Sweden', 'Switzerland', 'Syria', 'Taiwan', 'Tajikistan', 'Tanzania', 'Thailand', 'Togo', 'Tonga', 'Trinidad and Tobago', 'Tunisia', 'Turkey', 'Turkmenistan', 'Tuvalu', 'Uganda', 'Ukraine', 'United Arab Emirates', 'United Kingdom', 'United States', 'Uruguay', 'Uzbekistan', 'Vanuatu', 'Vatican City', 'Venezuela', 'Vietnam', 'Yemen', 'Zambia', 'Zimbabwe']\n",
        "    pattern = r'\\b(?:' + '|'.join(countries) + r')\\b|\\b[A-Z]{2}\\b'  # Country names or country abbreviations\n",
        "    matches = re.findall(pattern, text)\n",
        "    return matches\n",
        "\n",
        "# Function to extract organization names\n",
        "def extract_organization_names(text):\n",
        "    organizations = ['UN', 'United Nations', 'U.N.', 'World Health Organization', 'Amnesty International']\n",
        "    pattern = r'\\b(?:' + '|'.join(organizations) + r')\\b'\n",
        "    matches = re.findall(pattern, text)\n",
        "    return matches\n",
        "\n",
        "# Function to extract government official titles\n",
        "def extract_official_titles(text):\n",
        "    titles = ['President', 'Prime Minister', 'Secretary of State', 'Defense Minister']\n",
        "    pattern = r'\\b(?:' + '|'.join(titles) + r')\\b'\n",
        "    matches = re.findall(pattern, text)\n",
        "    return matches\n",
        "\n",
        "# Function to extract datetimes\n",
        "def extract_datetimes(text):\n",
        "    months = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
        "    pattern = r'\\b(?:' + '|'.join(months) + r')\\b|\\b\\d{1,2}(?:st|nd|rd|th)?(?:\\s(?:January|February|March|April|May|June|July|August|September|October|November|December))?\\b|\\b\\d{4}\\b'\n",
        "    matches = re.findall(pattern, text)\n",
        "    return matches\n",
        "\n",
        "# Test the functions on a sample sentence\n",
        "# sample_sentence = \"Afghan President Hamid Karzai says he has fired two 'high-ranking' Afghan officials who were spying for other countries.\"\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('NER_Dataset.csv')\n",
        "sentences = df['Word'].tolist()\n",
        "sample_sentence = sentences[10]\n",
        "print(sample_sentence)\n",
        "print(\"Person Names:\", extract_person_names(sample_sentence))\n",
        "print(\"Country Names:\", extract_country_names(sample_sentence))\n",
        "print(\"Organization Names:\", extract_organization_names(sample_sentence))\n",
        "print(\"Official Titles:\", extract_official_titles(sample_sentence))\n",
        "print(\"Datetimes:\", extract_datetimes(sample_sentence))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('NER_Dataset.csv')\n",
        "data = df['Word'].tolist()\n",
        "sentences = [data[sentence] for sentence in range(600)]\n",
        "sentences = \"Afghan President Hamid Karzai says he has fired two 'high-ranking' Afghan officials who were spying for other countries.\"\n",
        "\n",
        "# Function to extract named entities\n",
        "def extract_named_entities(sentences):\n",
        "    named_entities = []\n",
        "    for sentence in sentences:\n",
        "        # Join the tokens to form a sentence\n",
        "        # sentence_text = \"\".join(sentence)\n",
        "        # Process the sentence with spaCy\n",
        "        doc = nlp(sentence)\n",
        "        # Extract named entities\n",
        "        for ent in doc.ents:\n",
        "            named_entities.append((ent.text, ent.label_))\n",
        "    return named_entities\n",
        "\n",
        "# Extract named entities\n",
        "named_entities = extract_named_entities(sentences)\n",
        "\n",
        "# Define common government official titles\n",
        "government_titles_list = [\"President\", \"Vice President\", \"Prime Minister\", \"Minister\", \"Secretary\", \"Chancellor\", \"Ambassador\", \"Governor\", \"Senator\", \"Congressman\", \"Diplomat\"]\n",
        "\n",
        "# Filter named entities based on categories\n",
        "person_names = [entity[0] for entity in named_entities if entity[1] == 'PERSON']\n",
        "country_names = [entity[0] for entity in named_entities if entity[1] == 'GPE']\n",
        "organization_names = [entity[0] for entity in named_entities if entity[1] == 'ORG']\n",
        "government_titles = [entity[0] for entity in named_entities if entity[1] == 'TITLE' and any(title in entity[0] for title in government_titles_list)]\n",
        "datetimes = [entity[0] for entity in named_entities if entity[1] == 'DATE']\n",
        "\n",
        "# Print the extracted named entities\n",
        "print(\"Person Names:\", person_names)\n",
        "print(\"Country Names:\", country_names)\n",
        "print(\"Organization Names:\", organization_names)\n",
        "print(\"Government Official Titles:\", government_titles)\n",
        "print(\"Datetimes:\", datetimes)\n",
        "\n",
        "# Iterate over named entities\n",
        "for entity, label in named_entities:\n",
        "    print(f\"Potential Title: {entity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92_VN9ww-XUo",
        "outputId": "0ec67cdb-a495-475d-9a70-4e0eb0cd79ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person Names: []\n",
            "Country Names: []\n",
            "Organization Names: []\n",
            "Government Official Titles: []\n",
            "Datetimes: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "\n",
        "# Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('NER_Dataset.csv')\n",
        "data = df['Word'].tolist()\n",
        "sentences = [data[sentence] for sentence in range(50)]\n",
        "\n",
        "# Function to extract named entities\n",
        "def extract_named_entities(sentences):\n",
        "    named_entities = []\n",
        "    for sentence in sentences:\n",
        "        # Join the tokens to form a sentence\n",
        "        sentence_text = \"\".join(sentence)\n",
        "        # Process the sentence with spaCy\n",
        "        doc = nlp(sentence_text)\n",
        "        # Extract named entities\n",
        "        for ent in doc.ents:\n",
        "            named_entities.append((ent.text, ent.label_))\n",
        "    return named_entities\n",
        "\n",
        "# Extract named entities\n",
        "named_entities = extract_named_entities(sentences)\n",
        "\n",
        "# Define common government official titles\n",
        "government_titles_list = [\"President\", \"Vice President\", \"Prime Minister\", \"Minister\", \"Secretary\", \"Chancellor\", \"Ambassador\", \"Governor\", \"Senator\", \"Congressman\", \"Diplomat\"]\n",
        "\n",
        "# Filter named entities based on categories\n",
        "person_names = [entity[0] for entity in named_entities if entity[1] == 'PERSON']\n",
        "country_names = [entity[0] for entity in named_entities if entity[1] == 'GPE']\n",
        "organization_names = [entity[0] for entity in named_entities if entity[1] == 'ORG']\n",
        "government_titles = [entity[0] for entity in named_entities if entity[1] == 'TITLE' and any(title.lower() in entity[0] for title in government_titles_list)]\n",
        "datetimes = [entity[0] for entity in named_entities if entity[1] == 'DATE']\n",
        "\n",
        "# Print the extracted named entities\n",
        "print(\"Person Names:\", person_names)\n",
        "print(\"Country Names:\", country_names)\n",
        "print(\"Organization Names:\", organization_names)\n",
        "print(\"Government Official Titles:\", government_titles)\n",
        "print(\"Datetimes:\", datetimes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc-3G22H4vvi",
        "outputId": "a3b8d596-edde-441b-97d2-949947a58742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Person Names: ['Orakzai', 'Waziristan', 'Siniora', 'Rafik', 'Hariri', 'Detlev', 'Mehlis', 'Ariel', 'Sharon', 'Sharon', 'Sharon', 'Sharon', 'Ansari', 'Paul', 'Allen', 'Haarde', 'Dogg', 'Broadus', 'Dogg', 'Dogg']\n",
            "Country Names: ['London', 'Iraq', 'U.S.', 'Aceh', 'Indonesia', 'India', 'Beirut', 'Beirut', 'Lebanon', 'Syria', 'Damascus', 'Iceland', 'Rutan', 'U.S.', 'Missouri', 'California', 'Korea', 'Pyongyang', 'Korea', 'Indonesia', 'U.S.', 'Indonesia', 'U.S.', 'Britain', 'London', 'California', 'Iceland']\n",
            "Organization Names: ['Taliban', 'U.N.', 'Egeland', 'Egeland', 'U.N.', 'Fouad', 'U.N.', 'SpaceShipOne', 'SpaceShipOne', 'SpaceShipOne', 'Microsoft']\n",
            "Government Official Titles: []\n",
            "Datetimes: ['Wednesday', 'Saturday', 'Sunday', \"'week'\", 'Friday', 'Friday', 'February', 'Friday', 'Thursday', 'December', \"'week'\", 'Saturday', \"two-week'\", 'September', 'October', \"'week'\", 'Monday', 'Tuesday', 'March', 'December', 'Thursday']\n"
          ]
        }
      ]
    }
  ]
}